{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize\n",
    "\n",
    "The purpose of this notebook is to quantize the model in order to reduce the file size of the model required for deployment. It will also evaluate the new model on the test data, and compare the model metrics before and after quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vars\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "MODEL_NAME = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "model = load_model(os.path.join(ROOT_DIR, f'{MODEL_NAME}.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/3h/stbwvzgs2pg2db9fz55n3yg80000gn/T/tmpsrrha_mq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/3h/stbwvzgs2pg2db9fz55n3yg80000gn/T/tmpsrrha_mq/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "2023-10-09 16:03:05.896835: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-10-09 16:03:05.896848: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-10-09 16:03:05.897601: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/3h/stbwvzgs2pg2db9fz55n3yg80000gn/T/tmpsrrha_mq\n",
      "2023-10-09 16:03:05.899957: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-10-09 16:03:05.899963: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/3h/stbwvzgs2pg2db9fz55n3yg80000gn/T/tmpsrrha_mq\n",
      "2023-10-09 16:03:05.904268: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
      "2023-10-09 16:03:05.906658: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-10-09 16:03:06.187941: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/3h/stbwvzgs2pg2db9fz55n3yg80000gn/T/tmpsrrha_mq\n",
      "2023-10-09 16:03:06.212949: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 315349 microseconds.\n",
      "2023-10-09 16:03:06.238583: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the model to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Enable post-training quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Convert the model to a quantized TensorFlow Lite model\n",
    "quantized_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model to file\n",
    "\n",
    "with open('quantized_model.tflite', 'wb') as f:\n",
    "    f.write(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset('test_data.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    input_data, true_label = sample\n",
    "\n",
    "    quantized_model.set_tensor(input_index, input_data)\n",
    "\n",
    "    quantized_model.invoke()\n",
    "\n",
    "    output_data = quantized_model.get_tensor(output_index)\n",
    "\n",
    "    predictions.append(output_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
